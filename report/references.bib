
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @string{AMSTrans = "American Mathematical Society Translations"}
 @string{AMSTrans = "Amer. Math. Soc. Transl."}
 @string{BullAMS = "Bulletin of the American Mathematical Society"}
 @string{BullAMS = "Bull. Amer. Math. Soc."}
 @string{ProcAMS = "Proceedings of the American Mathematical Society"}
 @string{ProcAMS = "Proc. Amer. Math. Soc."}
 @string{TransAMS = "Transactions of the American Mathematical Society"}
 @string{TransAMS = "Trans. Amer. Math. Soc."}

 %ACM
 @string{CACM = "Communications of the {ACM}"}
 @string{CACM = "Commun. {ACM}"}
 @string{CompServ = "Comput. Surveys"}
 @string{JACM = "J. ACM"}
 @string{ACMMathSoft = "{ACM} Transactions on Mathematical Software"}
 @string{ACMMathSoft = "{ACM} Trans. Math. Software"}
 @string{SIGNUM = "{ACM} {SIGNUM} Newsletter"}
 @string{SIGNUM = "{ACM} {SIGNUM} Newslett."}

 @string{AmerSocio = "American Journal of Sociology"}
 @string{AmerStatAssoc = "Journal of the American Statistical Association"}
 @string{AmerStatAssoc = "J. Amer. Statist. Assoc."}
 @string{ApplMathComp = "Applied Mathematics and Computation"}
 @string{ApplMathComp = "Appl. Math. Comput."}
 @string{AmerMathMonthly = "American Mathematical Monthly"}
 @string{AmerMathMonthly = "Amer. Math. Monthly"}
 @string{BIT = "{BIT}"}
 @string{BritStatPsych = "British Journal of Mathematical and Statistical
                           Psychology"}
 @string{BritStatPsych = "Brit. J. Math. Statist. Psych."}
 @string{CanMathBull = "Canadian Mathematical Bulletin"}
 @string{CanMathBull = "Canad. Math. Bull."}
 @string{CompApplMath = "Journal of Computational and Applied Mathematics"}
 @string{CompApplMath = "J. Comput. Appl. Math."}
 @string{CompPhys = "Journal of Computational Physics"}
 @string{CompPhys = "J. Comput. Phys."}
 @string{CompStruct = "Computers and Structures"}
 @string{CompStruct = "Comput. \& Structures"}
 @string{CompJour = "The Computer Journal"}
 @string{CompJour = "Comput. J."}
 @string{CompSysSci = "Journal of Computer and System Sciences"}
 @string{CompSysSci = "J. Comput. System Sci."}
 @string{Computing = "Computing"}
 @string{ContempMath = "Contemporary Mathematics"}
 @string{ContempMath = "Contemp. Math."}
 @string{Crelle = "Crelle's Journal"}
 @string{GiornaleMath = "Giornale di Mathematiche"}
 @string{GiornaleMath = "Giorn. Mat."} % didn't find in AMS MR., ibid.

 %IEEE
 @string{Computer = "{IEEE} Computer"}
 @string{IEEETransComp = "{IEEE} Transactions on Computers"}
 @string{IEEETransComp = "{IEEE} Trans. Comput."}
 @string{IEEETransAC = "{IEEE} Transactions on Automatic Control"}
 @string{IEEETransAC = "{IEEE} Trans. Automat. Control"}
 @string{IEEESpec = "{IEEE} Spectrum"} % didn't find in AMS MR
 @string{ProcIEEE = "Proceedings of the {IEEE}"}
 @string{ProcIEEE = "Proc. {IEEE}"} % didn't find in AMS MR
 @string{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
                               Systems"}
 @string{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems"}

 @string{IMANumerAna = "{IMA} Journal of Numerical Analysis"}
 @string{IMANumerAna = "{IMA} J. Numer. Anal."}
 @string{InfProcLet = "Information Processing Letters"}
 @string{InfProcLet = "Inform. Process. Lett."}
 @string{InstMathApp = "Journal of the Institute of Mathematics and its
                         Applications"}
 @string{InstMathApp = "J. Inst. Math. Appl."}
 @string{IntControl = "International Journal of Control"}
 @string{IntControl = "Internat. J. Control"}
 @string{IntNumerEng = "International Journal for Numerical Methods in
                         Engineering"}
 @string{IntNumerEng = "Internat. J. Numer. Methods Engrg."}
 @string{IntSuper = "International Journal of Supercomputing Applications"}
 @string{IntSuper = "Internat. J. Supercomputing Applic."} % didn't find
%% in AMS MR
 @string{Kibernetika = "Kibernetika"}
 @string{JResNatBurStand = "Journal of Research of the National Bureau of
                             Standards"}
 @string{JResNatBurStand = "J. Res. Nat. Bur. Standards"}
 @string{LinAlgApp = "Linear Algebra and its Applications"}
 @string{LinAlgApp = "Linear Algebra Appl."}
 @string{MathAnaAppl = "Journal of Mathematical Analysis and Applications"}
 @string{MathAnaAppl = "J. Math. Anal. Appl."}
 @string{MathAnnalen = "Mathematische Annalen"}
 @string{MathAnnalen = "Math. Ann."}
 @string{MathPhys = "Journal of Mathematical Physics"}
 @string{MathPhys = "J. Math. Phys."}
 @string{MathComp = "Mathematics of Computation"}
 @string{MathComp = "Math. Comp."}
 @string{MathScand = "Mathematica Scandinavica"}
 @string{MathScand = "Math. Scand."}
 @string{TablesAidsComp = "Mathematical Tables and Other Aids to Computation"}
 @string{TablesAidsComp = "Math. Tables Aids Comput."}
 @string{NumerMath = "Numerische Mathematik"}
 @string{NumerMath = "Numer. Math."}
 @string{PacificMath = "Pacific Journal of Mathematics"}
 @string{PacificMath = "Pacific J. Math."}
 @string{ParDistComp = "Journal of Parallel and Distributed Computing"}
 @string{ParDistComp = "J. Parallel and Distrib. Comput."} % didn't find
%% in AMS MR
 @string{ParComputing = "Parallel Computing"}
 @string{ParComputing = "Parallel Comput."}
 @string{PhilMag = "Philosophical Magazine"}
 @string{PhilMag = "Philos. Mag."}
 @string{ProcNAS = "Proceedings of the National Academy of Sciences of the USA"}
 @string{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A."}
 @string{Psychometrika = "Psychometrika"}
 @string{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)"}
 @string{QuartMath = "Quart. J. Math. Oxford Ser. (2)"}
 @string{QuartApplMath = "Quarterly of Applied Mathematics"}
 @string{QuartApplMath = "Quart. Appl. Math."}
 @string{RevueInstStat = "Review of the International Statisical Institute"}
 @string{RevueInstStat = "Rev. Inst. Internat. Statist."}

 %SIAM
 @string{JSIAM = "Journal of the Society for Industrial and Applied Mathematics
                   "}
 @string{JSIAM = "J. Soc. Indust. Appl. Math."}
 @string{JSIAMB = "Journal of the Society for Industrial and Applied
                    Mathematics, Series B, Numerical Analysis"}
 @string{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal."}
 @string{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods"}
 @string{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods"}
 @string{SIAMAppMath = "{SIAM} Journal on Applied Mathematics"}
 @string{SIAMAppMath = "{SIAM} J. Appl. Math."}
 @string{SIAMComp = "{SIAM} Journal on Computing"}
 @string{SIAMComp = "{SIAM} J. Comput."}
 @string{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications"}
 @string{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl."}
 @string{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis"}
 @string{SIAMNumAnal = "{SIAM} J. Numer. Anal."}
 @string{SIAMReview = "{SIAM} Review"}
 @string{SIAMReview = "{SIAM} Rev."}
 @string{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical Computing"}
 @string{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput."}

 @string{SoftPracExp = "Software Practice and Experience"}
 @string{SoftPracExp = "Software Prac. Experience"} % didn't find in AMS MR
 @string{StatScience = "Statistical Science"}
 @string{StatScience = "Statist. Sci."}
 @string{Techno = "Technometrics"}
 @string{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
                              Physics"}
 @string{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys."}
 @string{VLSICompSys = "Journal of {VLSI} and Computer Systems"}
 @string{VLSICompSys = "J. {VLSI} Comput. Syst."}
 @string{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und Mechanik"}
 @string{ZAngewMathMech = "Z. Angew. Math. Mech."}
 @string{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik"}
 @string{ZAngewMathPhys = "Z. Angew. Math. Phys."}

% Publishers % ================================================= |

 @string{Academic = "Academic Press"}
 @string{ACMPress = "{ACM} Press"}
 @string{AdamHilger = "Adam Hilger"}
 @string{AddisonWesley = "Addison-Wesley"}
 @string{AllynBacon = "Allyn and Bacon"}
 @string{AMS = "American Mathematical Society"}
 @string{Birkhauser = "Birkha{\"u}ser"}
 @string{CambridgePress = "Cambridge University Press"}
 @string{Chelsea = "Chelsea"}
 @string{ClaredonPress = "Claredon Press"}
 @string{DoverPub = "Dover Publications"}
 @string{Eyolles = "Eyolles"}
 @string{HoltRinehartWinston = "Holt, Rinehart and Winston"}
 @string{Interscience = "Interscience"}
 @string{JohnsHopkinsPress = "The Johns Hopkins University Press"}
 @string{JohnWileySons = "John Wiley and Sons"}
 @string{Macmillan = "Macmillan"}
 @string{MathWorks = "The Math Works Inc."}
 @string{McGrawHill = "McGraw-Hill"}
 @string{NatBurStd = "National Bureau of Standards"}
 @string{NorthHolland = "North-Holland"}
 @string{OxfordPress = "Oxford University Press"}  %address Oxford or London?
 @string{PergamonPress = "Pergamon Press"}
 @string{PlenumPress = "Plenum Press"}
 @string{PrenticeHall = "Prentice-Hall"}
 @string{SIAMPub = "{SIAM} Publications"}
 @string{Springer = "Springer-Verlag"}
 @string{TexasPress = "University of Texas Press"}
 @string{VanNostrand = "Van Nostrand"}
 @string{WHFreeman = "W. H. Freeman and Co."}

%Entries

@article{Yuan2022,
 author = "Meng Yuan and Justin Zobel and Pauline Lin",
 title = "Measurement of clustering effectiveness for document collections",
 journal = "Information Retrieval Journal",
 volume = "25",
 month = jan,
 year = "2022",
 pages = "239--268",
 doi = "10.1007/s10791-021-09401-8",
 url = "https://doi.org/10.1007/s10791-021-09401-8",
 note = "",
}

@inproceedings{Yuan2021,
 author = "Meng Yuan and Pauline Lin and Justin Zobel",
 title = "Document Clustering vs Topic Models: A Case Study",
 booktitle = "Proceedings of the 25th Australasian Document Computing Symposium",
 series = "ADCS '21",
 articleno = "6",
 year = "2022",
 publisher = "Association for Computing Machinery",
 address = "New York, NY, USA",
 pages = "1-8",
 doi = "10.1145/3503516.3503527",
 url = "https://doi.org/10.1145/3503516.3503527",
}

@article{Cazals2019,
 author = "F. Cazals and D. Mazauric and R. Tetley and R. Watrigant",
 title = "Comparing Two Clusterings Using Matchings between Clusters of Clusters
          ",
 year = "2019",
 month = oct,
 journal = "ACM J. Exp. Algorithmics",
 volume = "24",
 pages = "1-41",
 doi = "10.1145/3345951",
 url = "https://doi.org/10.1145/3345951",
}

@inproceedings{MacAvaney2022,
 author = {MacAvaney, Sean and Tonellotto, Nicola and Macdonald, Craig},
 title = {Adaptive Re-Ranking with a Corpus Graph},
 year = {2022},
 isbn = {9781450392365},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3511808.3557231},
 doi = {10.1145/3511808.3557231},
 abstract = {Search systems often employ a re-ranking pipeline, wherein
             documents (or passages) from an initial pool of candidates are
             assigned new ranking scores. The process enables the use of
             highly-effective but expensive scoring functions that are not
             suitable for use directly in structures like inverted indices or
             approximate nearest neighbour indices. However, re-ranking pipelines
             are inherently limited by the recall of the initial candidate pool;
             documents that are not identified as candidates for re-ranking by
             the initial retrieval function cannot be identified. We propose a
             novel approach for overcoming the recall limitation based on the
             well-established clustering hypothesis. Throughout the re-ranking
             process, our approach adds documents to the pool that are most
             similar to the highest-scoring documents up to that point. This
             feedback process adapts the pool of candidates to those that may
             also yield high ranking scores, even if they were not present in the
             initial pool. It can also increase the score of documents that
             appear deeper in the pool that would have otherwise been skipped due
             to a limited re-ranking budget. We find that our Graph-based
             Adaptive Re-ranking (GAR) approach significantly improves the
             performance of re-ranking pipelines in terms of precision- and
             recall-oriented measures, is complementary to a variety of existing
             techniques (e.g., dense retrieval), is robust to its hyperparameters
             , and contributes minimally to computational and storage costs. For
             instance, on the MS MARCO passage ranking dataset, GAR can improve
             the nDCG of a BM25 candidate pool by up to 8\% when applying a
             monoT5 ranker.},
 booktitle = {Proceedings of the 31st ACM International Conference on
              Information \& Knowledge Management},
 pages = {1491--1500},
 numpages = {10},
 keywords = {neural re-ranking, clustering hypothesis},
 location = {Atlanta, GA, USA},
 series = {CIKM '22},
}
@inproceedings{Kulkarni2023,
 author = {Kulkarni, Hrishikesh and MacAvaney, Sean and Goharian, Nazli and
           Frieder, Ophir},
 title = {Lexically-Accelerated Dense Retrieval},
 year = {2023},
 isbn = {9781450394086},
 publisher = {Association for Computing Machinery},
 address = {New York, NY, USA},
 url = {https://doi.org/10.1145/3539618.3591715},
 doi = {10.1145/3539618.3591715},
 abstract = {Retrieval approaches that score documents based on learned dense
             vectors (i.e., dense retrieval) rather than lexical signals (i.e.,
             conventional retrieval) are increasingly popular. Their ability to
             identify related documents that do not necessarily contain the same
             terms as those appearing in the user's query (thereby improving
             recall) is one of their key advantages. However, to actually achieve
             these gains, dense retrieval approaches typically require an
             exhaustive search over the document collection, making them
             considerably more expensive at query-time than conventional lexical
             approaches. Several techniques aim to reduce this computational
             overhead by approximating the results of a full dense retriever.
             Although these approaches reasonably approximate the top results,
             they suffer in terms of recall -- one of the key advantages of dense
             retrieval. We introduce 'LADR' (Lexically-Accelerated Dense
             Retrieval), a simple-yet-effective approach that improves the
             efficiency of existing dense retrieval models without compromising
             on retrieval effectiveness. LADR uses lexical retrieval techniques
             to seed a dense retrieval exploration that uses a document proximity
             graph. Through extensive experiments, we find that LADR establishes
             a new dense retrieval effectiveness-efficiency Pareto frontier among
             approximate k nearest neighbor techniques. When tuned to take around
             8ms per query in retrieval latency on our hardware, LADR
             consistently achieves both precision and recall that are on par with
             an exhaustive search on standard benchmarks. Importantly, LADR
             accomplishes this using only a single CPU -- no hardware
             accelerators such as GPUs -- which reduces the deployment cost of
             dense retrieval systems.},
 booktitle = {Proceedings of the 46th International ACM SIGIR Conference on
              Research and Development in Information Retrieval},
 pages = {152â€“162},
 numpages = {11},
 keywords = {adaptive re-ranking, approximate k nearest neighbor, dense
             retrieval},
 location = {, Taipei, Taiwan, },
 series = {SIGIR '23},
}

@inproceedings{Frayling2024,
 author = {Frayling, Erlend MacAvaney, Sean Macdonald, Craig Ounis, Iadh},
 editor = {Goharian, Nazli Tonellotto, Nicola He, Yulan Lipani, Aldo McDonald,
           Graham Macdonald, Craig Ounis, Iadh},
 title = {Effective Adhoc Retrieval Through Traversal of a Query-Document Graph},
 booktitle = {Advances in Information Retrieval},
 abstract = {Adhoc retrieval is the task of effectively retrieving information
             for an end-user's information need, usually expressed as a textual
             query. One of the most well-established retrieval frameworks is the
             two-stage retrieval pipeline, whereby an inexpensive retrieval
             algorithm retrieves a subset of candidate documents from a corpus,
             and a more sophisticated (but costly) model re-ranks these
             candidates. A notable limitation of this two-stage framework is that
             the second stage re-ranking model can only re-order documents, and
             any relevant documents not retrieved from the corpus in the first
             stage are entirely lost to the second stage. A recently-proposed
             Adaptive Re-Ranking technique has shown that extending the candidate
             pool by traversing a document similarity graph can overcome this
             recall problem. However, this traversal technique is agnostic of the
             user's query, which has the potential to waste compute resources by
             scoring documents that are not related to the query. In this work,
             we propose an alternative formulation of the document similarity
             graph. Rather than using document similarities, we propose a
             weighted bipartite graph that consists of both document nodes and
             query nodes. This overcomes the limitations of prior Adaptive
             Re-Ranking approaches because the bipartite graph can be navigated
             in a manner that explicitly acknowledges the original user query
             issued to the search pipeline. We evaluate the effectiveness of our
             proposed framework by experimenting with the TREC Deep Learning
             track in a standard adhoc retrieval setting. We find that our
             approach outperforms state-of-the-art two-stage re-ranking pipelines
             , improving the nDCG@10 metric by 5.8{\%} on the DL19 test
             collection.},
 year = {2024},
 publisher = {Springer Nature Switzerland},
 address = {Cham},
 pages = {89--104},
 isbn = {978-3-031-56063-7},
}

