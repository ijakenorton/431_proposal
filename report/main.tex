\documentclass[sigconf,authorversion,nonacm]{acmart}
\usepackage{algorithm}
\usepackage{algorithmic}
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}


\begin{document}
\title{Re-Ranking Dense Retrieval}
\author{Jake Norton}

\affiliation{
	\institution{University of Otago}
	\city{Dunedin}
	\state{Otago}
	\country{New Zealand}
}

\email{norja159@student.otago.ac.nz}

%%http://ch/
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}

	This paper explores advanced retrieval techniques aimed at enhancing the efficiency and
	effectiveness of information retrieval systems through use of graph-based and latent feature
	methodologies. I analyze three distinct approaches: Lexically-Accelerated Dense Retrieval
	(LADR)\cite{ladr}, which utilizes latent features to optimize document retrieval; Adaptive
	Re-Ranking with a Corpus Graph (GAR)\cite{gar}, which dynamically adjusts document rankings
	within a corpus graph to improve relevance; and Adhoc Retrieval Through Traversal of a
	Query-Document Graph(QDGT)\cite{query-document}, which employs direct traversal methods to
	optimize query-specific document retrieval. I propose to build ontop of these by using the idea
	of document neighbourhoods from(QDGT)\cite{query-document} but using the LADR methodologies.

\end{abstract}


%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
\begin{CCSXML} <ccs2012> <concept> <concept_id>10002951.10003317.10003338.10010403</concept_id>
	<concept_desc>Information systems~Novelty in information retrieval</concept_desc>
	<concept_significance>500</concept_significance> </concept> </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Information systems~Novelty in information retrieval}
% %% Keywords. The author(s) should pick words that accurately describe
% %% the work being presented. Separate the keywords with commas.
\keywords{dense retrieval, approximate k nearest neighbour, adaptive re-ranking, neural re-ranking, clustering hypothesis}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
%% Make note of how the papers are related to one another
\section{Introduction}

Dense retrieval is gaining popularity, driven by the widespread adoption of pre-trained language
models such as BERT and GPT, which excel at learning semantic representations of words. This approach
involves several trade-offs, particularly noticeable in re-ranking pipelines where choices must be
made about the size of the initial ranked set and the computational budget allocated for further
re-ranking.

One of the key advantages of dense retrieval is its ability to improve recall by enabling queries
based on semantic meaning rather than exact lexical matches. However, this benefit comes at the cost
of increased computational demand during both indexing and query-time. Additionally, techniques that
use approximation approaches in the second re-ranking phase often suffer from diminished recall,
which is counterproductive to their primary objective.

Ideally, to surpass the performance of traditional models, an exhaustive search over all documents
would be performed. However, such a strategy scales linearly with the number of documents and allows
no pruning, in contrast to lexical approaches.

Enhancing recall remains a significant challenge in information retrieval. Alternative methods, such
as those employed by models like SPADE\cite{spade}, leverage query expansion to identify semantically similar
documents in the initial stage, potentially improving recall with reduced computational overhead at
query-time, though still requiring additional effort during indexing.

A prevalent technique in this field is the use of the top-ranked results from an initial query as a
"seed" to guide the re-ranking and exploration processes. This approach leverages the initial
results to refine and enhance subsequent retrieval efforts.

Supporting these techniques is the clustering hypothesis\cite{clustering_hypothesis}, which posits that documents similar in
content are likely to be relevant to the same queries. This hypothesis is fundamental to many
advanced exploration techniques in dense retrieval, as it allows for more efficient and effective
identification of relevant documents through clustering-based strategies.

\section{Lexically-Accelerated Dense Retrieval}

Lexically-Accelerated Dense Retrieval (LADR)\cite{ladr} uses lexical retrieval to seed dense retrieval
exploration, leveraging a document proximity graph. This method establishes a new
effectiveness-efficiency Pareto frontier, requiring only a CPU, which offers significant advantages
in terms of cost and ease of deployment.

Instead of developing an entirely new paradigm, LADR builds on traditional lexical techniques.
Specifically, it uses an initial ranking with BM25 to efficiently create a top-K document pool,
followed by re-ranking with semantically aware vector embeddings. This approach does not solve the
recall problem entirely, as the pool of documents is limited by the initial ranking, which primarily
improves the ordering of the already gathered pool, enhancing precision.

Broadly speaking, LADR uses the initial ranking to seed an exploration of semantically similar
documents in subsequent re-ranking rounds. This approach is similar to an idea proposed by
GAR\cite{gar} for finding additional documents to score when re-ranking. However, LADR differs by
exploring similar documents as a method of pruning the dense retrieval search space while
maintaining comparable performance to an exhaustive search.

Documents are indexed using a standard lexical retrieval model. LADR employs two strategies:

\begin{algorithm}
	\caption{Proactive Algorithm}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} \( q \) query, \( n \) seed set size, \( k \) number of neighbours
		\STATE \textbf{Output:} \( R1 \) dense retrieval results
		\STATE let \( R0 \leftarrow \) obtain \( n \) documents using the query input into an initial model
		\STATE let \( P \leftarrow R0 \)
		\STATE let \( R1 \leftarrow 0 \)
		\STATE \( R1 \leftarrow R1 \cup \) top-\( k \) neighbours
		\STATE \( R \leftarrow \) top ranked docs from \( R1 \) with respect to \( q \)
	\end{algorithmic}
\end{algorithm}

\begin{enumerate}

	\item Obtain n documents using the query input into a BM25 model as the seed.
	\item Expand the document pool by unioning the seed documents with the K nearest neighbours of those documents.
	\item Store the final results of the expansion in R1, providing both the ranking and the expanded pool.

\end{enumerate}

\begin{algorithm}
	\caption{Adaptive Algorithm}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} \( q \) query, \( n \) seed set size, \( c \) exploration depth, \( k
		\) number of neighbours
		\STATE \textbf{Output:} \( R1 \) dense retrieval results
		\STATE let \( R0 \leftarrow \) obtain \( n \) documents using the query input into an initial model
		\STATE let \( R \leftarrow \) second stage scoring of seed documents \(R0\)
		\STATE let \( N \leftarrow\) top ranked \( c  \) from \(R \), \(c \) neighbours
		\STATE let \( N \leftarrow\) skip documents in R that have been seen
		\WHILE{\(N \not= 0\) }
		\STATE let \( R \leftarrow \) Score and add neighbours
		\STATE let \( N \leftarrow\) top-\( c  \) from \(R \), \(c \) neighbours
		\ENDWHILE
	\end{algorithmic}
\end{algorithm}

\begin{enumerate}

	\item Obtain n documents using the query input into a BM25 model as the seed.

	\item Score the seeds.

	\item Expand the document pool by unioning the seed documents with the K nearest neighbours of
	      those documents, skipping previously seen documents.

	\item Store the final results of the expansion in R1, providing both the ranking and the
	      expanded pool.

\end{enumerate}

\subsection{Research Questions}

\begin{itemize}

	\item \textbf{R1:} How does LADR compare with other approximate nearest neighbour techniques in
	      dense retrieval w.r.t performance and computational load?

	\item \textbf{R2:} What should be considered when choosing between proactive and adaptive LADR?

	\item \textbf{R3:} Does the nearest neighbour graph need to be constructed using an exhaustive
	      search? Or can it be optimized by using an approximation approach like Hierachical
	      Navicable Small World graphs?\cite{hnsw}

\end{itemize}


\subsection{Main Contributions}

LADR is a new technique to reduction computational load in dense retrieval. It is competitive with
other approximate nearest neighbour techniques, its parameters allow for a system which is flexible
and able to be optimized for specific use cases. State of the art for low latency approximate dense
retrieval.

\section{Adaptive Re-Ranking with a Corpus Graph}

The most effective retrieval strategies tend to be those that first retrieve a pool of candidate
documents. Cross-encoders model both the query and the document to compare the two, enhancing the
quality of re-ranking. However, re-ranking requires a budget, as it typically involves using
expensive ranking functions in the later stages.

GAR strategically allocates its ranking budget by focusing on neighbouring documents with the highest
current scores. This process is underpinned by a pre-computed corpus graph, which serves as a
directed map linking documents through a similarity function. To manage the graph's size and
complexity, each document is connected by a set number of edges, denoted as \(K\). In practice, the
selection process prioritizes the most similar neighbours based on this graph structure.

GAR is versatile, easily integrating into and enhancing a variety of re-ranking pipelines. Its graph
traversal technique is effective even with simple models, leveraging only lexical signals (BM25) for
initial retrieval and similarity assessments, this system maintains strong performance
compared to other more expensive models.

\begin{algorithm}
	\caption{Re-ranking Algorithm}
	\begin{algorithmic}[1]
		\STATE \textbf{input:} query \(q\), \(budget\), corpus graph \(g\)
		\STATE \textbf{output:} ranked list \(R1\)
		\STATE let \(R0 \leftarrow\) obtain \(n\) documents using the query input into an initial model
		\STATE let \(b \leftarrow\) batch size
		\STATE let \(P \leftarrow R2 \)
		\STATE let \(R1 \leftarrow 0 \)
		\STATE let \(F \leftarrow 0 \)

		\WHILE{\(R1 <\) \(budget\)}
		\STATE \(B \leftarrow\) re-rank top \(b\) documents from \(P\) using second stage scoring function
		\STATE Add batch to \(R1\) by taking the union of \(R1\) and top \(B\)
		\STATE \(R0 \leftarrow\) discard the batch from the initial ranking
		\STATE \(F \leftarrow\) Discard batch from frontier
		\STATE \(F \leftarrow\) Update frontier with neighbours of \(B\) using the precalculated corpus graph \(G\)
		\STATE \(P \leftarrow\) Alternate between initial ranking and the frontier
		\ENDWHILE

		\STATE \(R1 \leftarrow\) backfill \(R1\) with remaining items if the budget did not allow the algorithm to finish ranking
	\end{algorithmic}
\end{algorithm}

Alternating the previous ranked set P between the frontier allows for a balanced approach between
ranking the documents from the initial ranking and their neighbours. This strategy allows for
documents to be found which can be multiple graph nodes away from the origin document.

\subsection{Research Questions}

\begin{itemize}

	\item How does GAR compare with other re-ranking techniques in dense retrieval w.r.t
	      performance and computational load?

	\item What is the computational overhead?

	\item Does GAR improve other neural information retrieval systems?

\end{itemize}


\subsection{Main Contributions}

Provides a novel technique which uses a feedback loop to efficiently re-rank documents. This
technique can improve precision and recall when added to many other re-ranking pipelines. It uses a
corpus graph structure to guide the exploration process. This can allow documents to be ranked that
would previously have been missed due to budget constraints. It is still an effective technique when
used with a low cost similarity scoring model like BM25 as well as a range of other pipelines/
models.

\section{Effective Adhoc Retrieval Through Traversal of a Query-Document Graph}

GAR\cite{gar} does not take into account the users query itself, relying on the document similarity mapping to
higher match towards a query. Utilising a query document graph allows for the ranking to focus on
documents that are specific to the query itself, instead of to the parent document.

This technique uses a bipartite graph of documents to queries, Query-Document
Graph(QDG)\cite{query-document}. This is made such that there are no document-document connections,
similar documents are only connected through queries. This allows for the end-users query to be
mapped more directly to the precalculated graph. The issue with this approach in general is that it
requires there to be queries for each document, something which does not exist for any new corpus.
In this paper, Doc2Query\cite{doc2query} is used as generative a model to create queries from a given document.
These are then filtered on relevance to only keep the highest quality queries.

Another challenge is how it is possible to calculate the nearest documents to a document in
question, when it has to be accessed indirectly through a query. We can estimate the similarity
of two documents by getting the product of the relevance scores with respect to the common query. In
addition, as we no longer need the original document, we can find the similarity between the
hypothesised query q\('\) and the users original query with the potential documents.
So :
\[
	\text{Similarity}(d_1, d_2 \mid q_{\text{user}}) = \text{Similarity}(q_{\text{user}}, q') \times
	\text{Relevance}(q', d_2)
\]

There are a few different approaches posited in QDGT\cite{query-document}.First approach is to use
the same algorithm as GAR\cite{gar} , but substituting the corpus graph with the QDG, adding similar
documents found with the above method and expanding the frontier in the same way.

The second approach is to use the candidate queries from a given candidate query as a neighbourhood.
Then to see which candidate query has the highest relevance, rank a smaller subset of each query
neighbourhood to decide which query neighbourhood should be prioritized. This way resources are
spent on documents with the highest likelihood of relevance. This technique is called Reverted
Adaptive Retrieval(RAR). This approach also adds potential for interpretability, by mapping the
candidate queries which are picked in the frontier expansion phase.

Resource selection, takes the previous approach further, the subset of the documents which have
been ranked around a candidate query are averaged. Then these neighbourhoods are added to the new
frontier as prioritized neighbourhoods. This way neighbourhoods with the highest likelihood of
relevance are picked to be fully ranked in the next batch of ranking, as well as reducing the chance
of wasting resources on documents that aren't relevant.

\subsection{Research Questions}

\begin{itemize}
	\item Do the new techniques stated allow for more relevant documents to be found?
	\item Do these techniques improve performance with a lower re-ranking budget?
\end{itemize}


\subsection{Main Contributions}

This paper presents a new method to rank relevancy of documents based on the inverse of the clustering
hypothesis, that "documents relevant to the same query are similar"\cite{query-document}. The QDG can be
used by multiple re-ranking pipelines and show an increase in the nDCG@10 metric compared to others
in the same class. All three approaches stated, are competitive against both re-ranking pipeline
baselines as well as against full dense retrieval baselines.

\section{Relations between the papers}

All three papers aim to improve retrieval performance without sacrificing computational efficiency.
LADR and GAR share a strategy of using initial rankings to inform more complex re-ranking
strategies, while QDG introduces a similar concept but incorporates the query more directly into the
re-ranking process.

Both GAR and QDG utilize graph-based approaches, albeit in different forms (corpus graph vs.
query-document graph), highlighting a trend towards exploiting relational data for information
retrieval. These approaches suggest that graph-based methods are becoming crucial in navigating the
complexities of modern datasets.

There is potential for integrating the methodologies from these papers. For example, the graph-based
approaches of GAR and QDG could be combined with LADR's lexically-seeded retrieval to create a
hybrid system that leverages both document and query relations effectively.

Each paper balances exploration (finding new, relevant documents) and exploitation (utilizing known
good documents) differently. LADR begins with a traditional method and expands, GAR refines and
focuses using existing scores within a graph, and QDG explores relations through queries, suggesting
different strategies for managing the trade-offs between breadth and depth in search strategies.

\section{Research Q1}

Can the use of the resource gathering seen in QDGT\cite{query-document} be used to improve the
recall of both Proactive and Adaptive LADR?

\section{Research Q2}

What is the optimal neighbourhood sample size for achieving the highest retrieval performance in the
modified LADR model that incorporates neighbourhood-based ranking?

\subsection{Experiment}

It makes sense to answer both research questions in one experiment as the neighbourhood size will
also be relative to all the other parameters, and so needs to be tested at the same time.

\subsubsection{Objective}

To test the effectiveness of a modified LADR method where a subset of ranked neighbours from each
seed document is added to the document pool for ranking, aiming to enhance the efficiency and
the recall of the retrieval process.

To determine the most effective neighbourhood sample size that balances retrieval accuracy and
computational efficiency in the modified LADR model.

\subsubsection{Hypothesis}

Ranking and selecting a subset of neighbours before adding them to the pool of new documents to be
ranked will lead to more relevant document retrieval and therefore more budget efficient ranking.
Improving the recall of the lexically-accelerated dense retrieval process. I expect that smaller
budgets will benefit more from smaller neighbourhood sizes.

\subsubsection{Datasets}

As described in LADR\cite{ladr} I will also be using the same datasets to benchmark against.

\begin{itemize}
	\item TREC DL 2019
	\item TREC DL 2020
	\item MS MARCO small
\end{itemize}

\subsection{Procedure}

\textbf{Baseline Setup}

Collect data on average sizes of clusters surrounding documents to ascertain reasonable values, for
use as sample sizes when sampling the seed document neighbourhoods.

\begin{itemize}

	\item Use the standard LADR system as per the original paper, as implemented
	      on GitHub\footnote{\url{https://github.com/Georgetown-IR-Lab/ladr?tab=readme-ov-file}}.

	\item Perform the suite of experiments as seen in the LADR paper to establish baseline metrics
	      for recall, precision, and computational efficiency on our system.

\end{itemize}

\subsection{Experiment Execution}
\textbf{Modified Setup}
\begin{itemize}
	\item Modify the LADR algorithm to incorporate the ranking of neighbours. Specifically, for each seed document identified in the initial retrieval:
	      \begin{itemize}
		      \item For each parameter from the LADR implementation, i.e budget, cluster size,
		            neighbourhood size.
		      \item Retrieve a set of neighbouring documents
		      \item Rank these documents based on a predefined criterion (e.g., relevance scores, semantic similarity).
		      \item Select the top-ranked subset of these neighbours and add only this subset to \( R1 \).
	      \end{itemize}
\end{itemize}

\begin{itemize}
	\item Execute retrieval tasks using both the baseline and modified LADR systems.
	\item Record and compare the performance metrics (recall, precision, computational time).
\end{itemize}

\subsection{Expected Results}

It is expected that this selective ranking and addition process will prioritize the neighbourhoods
which are the most relevant. I would expect it to have more impact on the Proactive LADR, as I am
unsure how much this would effect the adaptive LADR given that it keeps going until convergence,
however given that a budget is involved, for budgets where it does not reach convergence it would
still be highly relevant.

\section{Conclusion}

In conclusion, this paper critically examines several advanced retrieval strategies that leverage
both graph-based and latent feature methodologies to enhance the efficiency and effectiveness of
information retrieval systems. By assessing three distinct methods—Lexically-Accelerated Dense
Retrieval (LADR), adaptive re-ranking with a corpus graph (GAR), and adhoc retrieval through a
query-document graph (QDGT)—this study underscores the potential of integrating these techniques to
optimize retrieval processes further. These methods collectively demonstrate how dynamic adjustment
of document relationships and direct traversal of query-document mappings can significantly refine
the relevance and precision of search results in large datasets. Future research could explore the
synergistic integration of these approaches, possibly leading to more sophisticated systems capable
of handling the increasing complexity of information retrieval tasks with higher computational
efficiency and improved accuracy.

\section*{}
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput

