introduction 

dense retrieval is becoming more popular as influenced by the general high adoption rate of
pre-trained language models such as BERT and T5 which are able to learn semantic representations of
words.

Recall is improved by the ability to make queries based on semantic meaning instead of a full
lexigraphical match. The main draw back of this is computation, at both indexing and query-time.
Other techniques like using an approximation approach in the second re-ranking phase suffer from a
deminishing of recall, the very point they are trying to solve.

To achieve better results than regular models it would be ideal to do an exhaustive search over all
documents. This scales linearly with the documents and does not allow any pruning, unlike
lexigraphical approachs.   

to get around this problem there have been several efforts made to find the approximate top
results....

Improving recall is the challenge of information expansion. Alternative ideas which attempt to
provide the same information need include models like SPADE which use query expansion to find
semantically similiar documents in the first stage. Expanding on the query allows for potentially
better recall with less computational cost at query-time. Though still relies on some additional
work during the indexing phase.

two types of neural models, interactive and non-interactive. The interaction being between the query
and the vector representation. Non-interactive models have static vector embeddings, interactive
models model the index and query.

Using the first rank as a seed is the most common technique being used currently.

As stated by the clustering hypothesis \ref{clustering_hypothesis} similiar documents tend to be related to the same queries,
this is the basis for many exploration techniques.

